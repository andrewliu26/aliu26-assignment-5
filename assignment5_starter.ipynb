{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "GehrTGM3PwNu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "31KwIpdgPwNw"
      },
      "outputs": [],
      "source": [
        "# Define the KNN class\n",
        "class KNN:\n",
        "    def __init__(self, k=3, distance_metric='euclidean'):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Store the training data for KNN.\n",
        "        \n",
        "        Parameters:\n",
        "        X (numpy array): Features of the training data\n",
        "        y (numpy array): Target values of the training data (churn or not)\n",
        "        \"\"\"\n",
        "        # Store training data\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class for each data point in X.\n",
        "        \n",
        "        Parameters:\n",
        "        X (numpy array): Features of the test data\n",
        "        \n",
        "        Returns:\n",
        "        numpy array: Predicted classes for the test data\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        \n",
        "        for x in X:\n",
        "            # Compute distances between x and all points in the training data\n",
        "            distances = [self.compute_distance(x, x_train) for x_train in self.X_train]\n",
        "            \n",
        "            # Get the indices of the k nearest neighbors\n",
        "            k_nearest_indices = np.argsort(distances)[:self.k]\n",
        "            \n",
        "            # Find the labels of the k nearest neighbors\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_nearest_indices]\n",
        "            \n",
        "            # Perform majority vote for classification\n",
        "            prediction = max(set(k_nearest_labels), key=k_nearest_labels.count)\n",
        "            predictions.append(prediction)\n",
        "        \n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict the probability of each class for each data point in X.\n",
        "        \n",
        "        Parameters:\n",
        "        X (numpy array): Features of the test data\n",
        "        \n",
        "        Returns:\n",
        "        numpy array: Probabilities for class 1 (churn) for the test data\n",
        "        \"\"\"\n",
        "        probabilities = []\n",
        "        \n",
        "        for x in X:\n",
        "            # Compute distances between x and all points in the training data\n",
        "            distances = [self.compute_distance(x, x_train) for x_train in self.X_train]\n",
        "            \n",
        "            # Get the indices of the k nearest neighbors\n",
        "            k_nearest_indices = np.argsort(distances)[:self.k]\n",
        "            \n",
        "            # Find the labels of the k nearest neighbors\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_nearest_indices]\n",
        "            \n",
        "            # Calculate the probability of belonging to class 1 (churn)\n",
        "            prob_class_1 = np.sum(k_nearest_labels) / self.k  # Since labels are binary (0 or 1)\n",
        "            probabilities.append(prob_class_1)\n",
        "        \n",
        "        return np.array(probabilities)\n",
        "\n",
        "    def compute_distance(self, X1, X2):\n",
        "        \"\"\"\n",
        "        Compute the distance between two points based on the specified metric.\n",
        "        \n",
        "        Parameters:\n",
        "        X1, X2 (numpy arrays): Points between which the distance is computed\n",
        "        \n",
        "        Returns:\n",
        "        float: The computed distance\n",
        "        \"\"\"\n",
        "        if self.distance_metric == 'euclidean':\n",
        "            return np.sqrt(np.sum((X1 - X2) ** 2))\n",
        "        elif self.distance_metric == 'manhattan':\n",
        "            return np.sum(np.abs(X1 - X2))\n",
        "        elif self.distance_metric == 'cosine':  # Added cosine similarity\n",
        "            dot_product = np.dot(X1, X2)\n",
        "            norm_X1 = np.linalg.norm(X1)\n",
        "            norm_X2 = np.linalg.norm(X2)\n",
        "            \n",
        "            return 1 - (dot_product / (norm_X1 * norm_X2))  # Convert similarity to distance\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(\"Unsupported distance metric. Use 'euclidean', 'manhattan', or 'cosine'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "npJplAAKPwNx"
      },
      "outputs": [],
      "source": [
        "# Define data preprocessing function\n",
        "def preprocess_data(train_path, test_path):\n",
        "    # Load the data\n",
        "    train_data = pd.read_csv(train_path)\n",
        "    test_data = pd.read_csv(test_path)\n",
        "    \n",
        "    # Handle missing values\n",
        "    # Fill missing numerical values with the mean\n",
        "    numerical_columns = train_data.select_dtypes(include=[np.number]).columns\n",
        "    \n",
        "    # Exclude 'Exited' from numerical columns for test data\n",
        "    numerical_columns_test = [col for col in numerical_columns if col != 'Exited']  \n",
        "    \n",
        "    train_data[numerical_columns] = train_data[numerical_columns].fillna(train_data[numerical_columns].mean())\n",
        "    test_data[numerical_columns_test] = test_data[numerical_columns_test].fillna(test_data[numerical_columns_test].mean())\n",
        "\n",
        "    # Handle categorical variables\n",
        "    # Convert categorical variables (Geography, Gender) into dummy variables (one-hot encoding)\n",
        "    categorical_columns = ['Geography', 'Gender']\n",
        "    train_data = pd.get_dummies(train_data, columns=categorical_columns, drop_first=True)\n",
        "    test_data = pd.get_dummies(test_data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Ensure the same columns exist in train and test data after encoding (handle missing categories)\n",
        "    train_columns = set(train_data.columns)\n",
        "    test_columns = set(test_data.columns)\n",
        "    missing_cols_in_test = train_columns - test_columns\n",
        "    missing_cols_in_train = test_columns - train_columns\n",
        "    \n",
        "    for col in missing_cols_in_test:\n",
        "        test_data[col] = 0\n",
        "    \n",
        "    for col in missing_cols_in_train:\n",
        "        train_data[col] = 0\n",
        "    \n",
        "    # Scale numerical features\n",
        "    features_to_scale = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
        "    for feature in features_to_scale:\n",
        "        train_mean = train_data[feature].mean()\n",
        "        train_std = train_data[feature].std()\n",
        "        \n",
        "        train_data[feature] = (train_data[feature] - train_mean) / train_std\n",
        "        test_data[feature] = (test_data[feature] - train_mean) / train_std  # Use train mean and std for scaling test set\n",
        "\n",
        "    # Separate features and target variable\n",
        "    X_train = train_data.drop(columns=['CustomerId', 'Surname', 'Exited'])\n",
        "    y_train = train_data['Exited']\n",
        "    \n",
        "    # Test data might not have 'Exited', so exclude it from the test set handling\n",
        "    X_test = test_data.drop(columns=['CustomerId', 'Surname'], errors='ignore')\n",
        "    \n",
        "    return X_train, y_train, X_test, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "6TN8H2tkPwNy"
      },
      "outputs": [],
      "source": [
        "# Define cross-validation function\n",
        "def cross_validate(X, y, knn, n_splits=5):\n",
        "    \"\"\"\n",
        "    Perform cross-validation on the provided KNN model.\n",
        "    \n",
        "    Parameters:\n",
        "    X (numpy array): Features of the dataset\n",
        "    y (numpy array): Target labels of the dataset\n",
        "    knn (KNN object): The KNN model to validate\n",
        "    n_splits (int): Number of cross-validation splits (default is 5)\n",
        "    \n",
        "    Returns:\n",
        "    numpy array: Array of ROC AUC scores for each split\n",
        "    \"\"\"\n",
        "    # Shuffle the data randomly before splitting\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    # Use .iloc to access rows by integer location\n",
        "    X, y = X.iloc[indices], y.iloc[indices] # Use .iloc for integer-based indexing\n",
        "    \n",
        "    # Split the data into `n_splits` folds\n",
        "    fold_size = X.shape[0] // n_splits\n",
        "    roc_auc_scores = []\n",
        "\n",
        "    for i in range(n_splits):\n",
        "        # Define start and end of the fold\n",
        "        start = i * fold_size\n",
        "        end = (i + 1) * fold_size if i != n_splits - 1 else X.shape[0]\n",
        "        \n",
        "        # Create training and validation sets\n",
        "        X_val, y_val = X.iloc[start:end], y.iloc[start:end] # Use .iloc for integer-based indexing\n",
        "        X_train = pd.concat([X.iloc[:start], X.iloc[end:]], axis=0) # Use .iloc for integer-based indexing and pd.concat\n",
        "        y_train = pd.concat([y.iloc[:start], y.iloc[end:]], axis=0) # Use .iloc for integer-based indexing and pd.concat\n",
        "        \n",
        "        # Train the model on the training set\n",
        "        knn.fit(X_train.values, y_train.values) # Convert to NumPy arrays\n",
        "        \n",
        "        # Predict probabilities for the validation set\n",
        "        y_pred = knn.predict(X_val.values) # Convert to NumPy array\n",
        "        \n",
        "        # Compute the ROC AUC score\n",
        "        roc_auc = roc_auc_score(y_val.values, y_pred)  # Convert to NumPy array\n",
        "        roc_auc_scores.append(roc_auc)\n",
        "\n",
        "    return np.array(roc_auc_scores)\n",
        "\n",
        "def roc_auc_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the ROC AUC score.\n",
        "    \n",
        "    Parameters:\n",
        "    y_true (numpy array): True binary labels\n",
        "    y_pred (numpy array): Predicted binary labels or probabilities\n",
        "    \n",
        "    Returns:\n",
        "    float: ROC AUC score\n",
        "    \"\"\"\n",
        "    # Sort by predicted probabilities\n",
        "    sorted_indices = np.argsort(y_pred)\n",
        "    y_true_sorted = y_true[sorted_indices]\n",
        "    \n",
        "    # Compute true positive rate (TPR) and false positive rate (FPR)\n",
        "    P = np.sum(y_true)  # Positive samples\n",
        "    N = len(y_true) - P  # Negative samples\n",
        "    tpr = np.cumsum(y_true_sorted) / P  # True positive rate\n",
        "    fpr = np.cumsum(1 - y_true_sorted) / N  # False positive rate\n",
        "    \n",
        "    # Compute ROC AUC using trapezoidal rule\n",
        "    auc = np.trapz(tpr, fpr)\n",
        "    return auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwcpz3XePwNy",
        "outputId": "7f51021f-99ea-4a95-ba3a-2344c51bfd9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=5, metric=euclidean, ROC AUC: 0.4783215531073274\n",
            "k=5, metric=manhattan, ROC AUC: 0.4631602593362307\n",
            "k=5, metric=cosine, ROC AUC: 0.22542334240343473\n",
            "k=7, metric=euclidean, ROC AUC: 0.499042168568114\n",
            "k=7, metric=manhattan, ROC AUC: 0.47297118997315335\n",
            "k=7, metric=cosine, ROC AUC: 0.2185755374193577\n",
            "k=9, metric=euclidean, ROC AUC: 0.49225382862815314\n",
            "k=9, metric=manhattan, ROC AUC: 0.48999203244049755\n",
            "k=9, metric=cosine, ROC AUC: 0.21782646777265388\n",
            "k=13, metric=euclidean, ROC AUC: 0.4992664858410921\n",
            "k=13, metric=manhattan, ROC AUC: 0.49922940058316134\n",
            "k=13, metric=cosine, ROC AUC: 0.22445252329455964\n",
            "Best parameters: k=13, distance_metric=euclidean, ROC AUC=0.4992664858410921\n",
            "Cross-validation scores: [0.50367596 0.50462345 0.49159629 0.50654061 0.49544699]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (13,) (12,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[122], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m.\u001b[39mvalues, y\u001b[38;5;241m.\u001b[39mvalues)  \u001b[38;5;66;03m# Convert to NumPy arrays\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(X_test\u001b[38;5;241m.\u001b[39mvalues)  \u001b[38;5;66;03m# Convert to NumPy array\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Save test predictions\u001b[39;00m\n\u001b[1;32m     68\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomerId\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExited\u001b[39m\u001b[38;5;124m'\u001b[39m: test_predictions})\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmissions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[119], line 33\u001b[0m, in \u001b[0;36mKNN.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Compute distances between x and all points in the training data\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     distances \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_distance(x, x_train) \u001b[38;5;28;01mfor\u001b[39;00m x_train \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Get the indices of the k nearest neighbors\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     k_nearest_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(distances)[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk]\n",
            "Cell \u001b[0;32mIn[119], line 86\u001b[0m, in \u001b[0;36mKNN.compute_distance\u001b[0;34m(self, X1, X2)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mCompute the distance between two points based on the specified metric.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mfloat: The computed distance\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39msum((X1 \u001b[38;5;241m-\u001b[39m X2) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mabs(X1 \u001b[38;5;241m-\u001b[39m X2))\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (13,) (12,) "
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning function\n",
        "def hyperparameter_tuning(X, y, k_values, distance_metrics, n_splits=5):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning to find the best k and distance metric.\n",
        "    \n",
        "    Parameters:\n",
        "    X (numpy array): Features of the dataset\n",
        "    y (numpy array): Target labels of the dataset\n",
        "    k_values (list): List of k values to try\n",
        "    distance_metrics (list): List of distance metrics to try\n",
        "    n_splits (int): Number of cross-validation splits\n",
        "    \n",
        "    Returns:\n",
        "    dict: Dictionary containing the best k, distance metric, and corresponding ROC AUC score\n",
        "    \"\"\"\n",
        "    best_score = -1\n",
        "    best_k = None\n",
        "    best_distance_metric = None\n",
        "    \n",
        "    for k in k_values:\n",
        "        for metric in distance_metrics:\n",
        "            # Create the KNN model with current hyperparameters\n",
        "            knn = KNN(k=k, distance_metric=metric)\n",
        "            \n",
        "            # Perform cross-validation\n",
        "            cv_scores = cross_validate(X, y, knn, n_splits=n_splits)\n",
        "            mean_cv_score = np.mean(cv_scores)\n",
        "            \n",
        "            print(f\"k={k}, metric={metric}, ROC AUC: {mean_cv_score}\")\n",
        "            \n",
        "            # Keep track of the best hyperparameters based on ROC AUC score\n",
        "            if mean_cv_score > best_score:\n",
        "                best_score = mean_cv_score\n",
        "                best_k = k\n",
        "                best_distance_metric = metric\n",
        "    \n",
        "    return {\n",
        "        \"best_k\": best_k,\n",
        "        \"best_distance_metric\": best_distance_metric,\n",
        "        \"best_score\": best_score\n",
        "    }\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y, X_test, _ = preprocess_data('train.csv', 'test.csv')\n",
        "\n",
        "# List of k values and distance metrics to explore\n",
        "k_values = [5, 7, 9, 13]\n",
        "distance_metrics = ['euclidean', 'manhattan', 'cosine']\n",
        "\n",
        "# Hyperparameter tuning\n",
        "best_params = hyperparameter_tuning(X, y, k_values, distance_metrics)\n",
        "print(f\"Best parameters: k={best_params['best_k']}, distance_metric={best_params['best_distance_metric']}, ROC AUC={best_params['best_score']}\")\n",
        "\n",
        "# Create KNN model with optimal hyperparameters\n",
        "knn = KNN(k=best_params['best_k'], distance_metric=best_params['best_distance_metric'])\n",
        "\n",
        "# Perform cross-validation (optional, already done during tuning)\n",
        "cv_scores = cross_validate(X, y, knn)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "\n",
        "# Train on full dataset with optimal hyperparameters\n",
        "knn.fit(X.values, y.values)  # Convert to NumPy arrays\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = knn.predict(X_test.values)  # Convert to NumPy array\n",
        "\n",
        "# Save test predictions\n",
        "pd.DataFrame({'id': pd.read_csv('test.csv')['CustomerId'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
